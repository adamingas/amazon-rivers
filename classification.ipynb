{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data manipulation\n",
    "Importing data, splitting it to train and test (because of the imbalance between white and black water it would be better if we set a minimum of black points in the test and train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "otudf =pd.read_csv(filepath_or_buffer=\"/home/spanashis/Documents/Stats/Project/amazon-rivers/otudf\",index_col=0)\n",
    "wwfdf = pd.read_csv(filepath_or_buffer=\"/home/spanashis/Documents/Stats/Project/amazon-rivers/wwfdf\",encoding=\"ISO-8859-1\",index_col = \"ID\")\n",
    "nmds20  = pd.read_csv(filepath_or_buffer=\"/home/spanashis/Documents/Stats/Project/amazon-rivers/nmds20dim\",index_col=0)\n",
    "# Black water sites. There are 21 black water sites and 143 white water\n",
    "blackindex = wwfdf[wwfdf[\"Water\"] == \"Black\"].index\n",
    "\n",
    "# Splitting training and test data\n",
    "np.random.seed(11235)\n",
    "X_train,X_test,y_train,y_test = train_test_split(otudf,wwfdf.Water,test_size = 0.2)\n",
    "np.random.seed(11235)\n",
    "Xn_train,Xn_test,y_train,y_test = train_test_split(nmds20,wwfdf.Water,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Logistic Regression with Lasso\n",
    "I am going to be trying both a frequentist and the bayesian approach. For the frequentist I will be using the sklearn package of python and for Bayesian I will use pymc3 to get the posterior distribution of the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Frequentist Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9393939393939394"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = LogisticRegression(penalty=\"l1\",class_weight=\"balanced\",multi_class=\"ovr\",solver= \"saga\",max_iter=10000)\n",
    "freq.fit(X_train,y_train)\n",
    "freq.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With saga solver and l1 penalty we get 94% accuracy on the test set\n",
    "with liblinear it fluctuates between 85% and 90%, but it doesn't always converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.16712035e-03, -8.85022297e-04, -8.74111074e-04,\n",
       "        -7.78210729e-04, -7.76417990e-04, -7.06168423e-04,\n",
       "        -5.05706379e-04, -4.90692464e-04, -4.77861073e-04,\n",
       "        -4.26898301e-04, -4.04111602e-04, -3.99245712e-04,\n",
       "        -3.78200407e-04, -3.19144610e-04, -3.07246344e-04,\n",
       "        -2.72271370e-04, -2.50966370e-04, -2.45427072e-04,\n",
       "        -2.42182202e-04, -2.23078358e-04, -1.94150415e-04,\n",
       "        -1.91808101e-04, -1.71450474e-04, -1.62989869e-04,\n",
       "        -1.52761619e-04, -1.33717603e-04, -1.12291243e-04,\n",
       "        -1.10998956e-04, -1.09928252e-04, -1.06512031e-04,\n",
       "        -1.05399908e-04, -1.04058807e-04, -1.01909792e-04,\n",
       "        -1.01478045e-04, -9.74960272e-05, -9.29283369e-05,\n",
       "        -9.06516177e-05, -8.97699763e-05, -8.30901108e-05,\n",
       "        -7.77029370e-05, -7.14585551e-05, -7.05492475e-05,\n",
       "        -6.92983826e-05, -6.47298329e-05, -5.26303206e-05,\n",
       "        -5.09353275e-05, -4.84162435e-05, -4.77956843e-05,\n",
       "        -4.69729041e-05, -4.55521320e-05, -4.46928885e-05,\n",
       "        -4.35922576e-05, -4.29970681e-05, -3.97287021e-05,\n",
       "        -3.96027195e-05, -3.70981961e-05, -3.62679782e-05,\n",
       "        -2.39954765e-05, -2.36026996e-05, -2.34467341e-05,\n",
       "        -2.23032125e-05, -2.13326424e-05, -1.95771653e-05,\n",
       "        -1.92222362e-05, -1.87798558e-05, -1.43819477e-05,\n",
       "        -1.30251336e-05, -1.28335124e-05, -1.24793038e-05,\n",
       "        -1.23516768e-05, -1.17831191e-05, -1.10935998e-05,\n",
       "        -1.10638943e-05, -7.56523586e-06, -7.26722501e-06,\n",
       "        -7.24219375e-06, -6.58083224e-06, -6.55133999e-06,\n",
       "        -5.41438227e-06, -5.27692607e-06, -5.05942390e-06,\n",
       "        -4.58493276e-06, -4.33264791e-06, -3.95154917e-06,\n",
       "        -3.79416226e-06, -3.71983936e-06, -3.35303291e-06,\n",
       "        -2.69235025e-06, -2.48301021e-06, -2.39125769e-06,\n",
       "        -2.19276913e-06, -1.92936837e-06, -1.32678391e-06,\n",
       "        -1.22412561e-06, -1.15906532e-06, -9.35683470e-07,\n",
       "        -2.83922278e-07, -2.38549526e-07, -1.17992738e-07,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.39192681e-09,  7.00223366e-08,\n",
       "         8.24508234e-08,  8.96959012e-08,  9.88286036e-08,\n",
       "         1.17342232e-07,  1.35408105e-07,  1.38233421e-07,\n",
       "         1.79024665e-07,  2.06037805e-07,  2.26688554e-07,\n",
       "         2.39623613e-07,  2.43213160e-07,  2.73896754e-07,\n",
       "         2.96804656e-07,  3.50146646e-07,  3.56048750e-07,\n",
       "         3.64680307e-07,  3.67561523e-07,  3.74091912e-07,\n",
       "         3.80532703e-07,  4.24097854e-07,  4.26027440e-07,\n",
       "         4.69257525e-07,  4.78792547e-07,  5.01243990e-07,\n",
       "         5.10022590e-07,  5.29298386e-07,  5.39214680e-07,\n",
       "         5.40605189e-07,  5.49671944e-07,  5.78863431e-07,\n",
       "         5.80228785e-07,  6.52699443e-07,  6.67692177e-07,\n",
       "         6.85272536e-07,  7.69699427e-07,  7.77507059e-07,\n",
       "         8.05126265e-07,  8.08620753e-07,  8.51787249e-07,\n",
       "         8.67992864e-07,  8.77927224e-07,  8.81580996e-07,\n",
       "         8.96628774e-07,  9.07838666e-07,  9.31201400e-07,\n",
       "         9.50628536e-07,  9.74481274e-07,  9.92600057e-07,\n",
       "         1.01274787e-06,  1.03857508e-06,  1.03857508e-06,\n",
       "         1.07697017e-06,  1.09656738e-06,  1.16279533e-06,\n",
       "         1.16809155e-06,  1.20891886e-06,  1.27489771e-06,\n",
       "         1.33638585e-06,  1.36571433e-06,  1.37050563e-06,\n",
       "         1.41095244e-06,  1.43246744e-06,  1.44957302e-06,\n",
       "         1.53826382e-06,  1.57158653e-06,  1.61330065e-06,\n",
       "         1.63830691e-06,  1.66257504e-06,  1.73642572e-06,\n",
       "         1.78718011e-06,  1.79545389e-06,  1.81523015e-06,\n",
       "         1.87995656e-06,  1.89936435e-06,  1.90129275e-06,\n",
       "         1.97794068e-06,  2.06183342e-06,  2.06424175e-06,\n",
       "         2.17728197e-06,  2.26961742e-06,  2.35314621e-06,\n",
       "         2.38595739e-06,  2.39903479e-06,  2.41548862e-06,\n",
       "         2.42154470e-06,  2.54756414e-06,  2.57500930e-06,\n",
       "         2.62730766e-06,  2.69347078e-06,  2.71718415e-06,\n",
       "         2.72693465e-06,  2.78776162e-06,  2.80849343e-06,\n",
       "         2.89508597e-06,  2.97690982e-06,  3.03541730e-06,\n",
       "         3.15973965e-06,  3.25141347e-06,  3.26390463e-06,\n",
       "         3.40894385e-06,  3.44415689e-06,  3.66600366e-06,\n",
       "         3.74032489e-06,  3.75725496e-06,  3.86083309e-06,\n",
       "         3.89216951e-06,  3.91585971e-06,  4.07314445e-06,\n",
       "         4.13389160e-06,  4.22440714e-06,  4.51043747e-06,\n",
       "         4.84061225e-06,  4.94905971e-06,  5.01992462e-06,\n",
       "         5.07653924e-06,  5.22865365e-06,  5.26621580e-06,\n",
       "         5.30231873e-06,  5.52990680e-06,  5.63763778e-06,\n",
       "         5.64722813e-06,  5.66231972e-06,  5.71051120e-06,\n",
       "         6.00808593e-06,  6.07201321e-06,  6.08555435e-06,\n",
       "         6.17030576e-06,  6.21939831e-06,  6.30452005e-06,\n",
       "         6.30733770e-06,  6.31005751e-06,  6.40113647e-06,\n",
       "         6.42155455e-06,  6.52883903e-06,  6.53873460e-06,\n",
       "         6.55491048e-06,  6.64561663e-06,  6.73387452e-06,\n",
       "         6.87760979e-06,  7.13461856e-06,  7.21756041e-06,\n",
       "         7.50642501e-06,  7.59385054e-06,  7.71207378e-06,\n",
       "         7.94183229e-06,  8.13900681e-06,  8.90605785e-06,\n",
       "         8.99773384e-06,  9.03219428e-06,  9.07080418e-06,\n",
       "         9.38071951e-06,  9.38082054e-06,  9.40674276e-06,\n",
       "         9.62412238e-06,  9.63261525e-06,  9.64962239e-06,\n",
       "         9.72089292e-06,  9.98466352e-06,  1.00776408e-05,\n",
       "         1.00897660e-05,  1.01821244e-05,  1.01905390e-05,\n",
       "         1.03111389e-05,  1.04140613e-05,  1.08103326e-05,\n",
       "         1.09269070e-05,  1.10470601e-05,  1.10673127e-05,\n",
       "         1.12954809e-05,  1.16270910e-05,  1.17447565e-05,\n",
       "         1.18538331e-05,  1.22281120e-05,  1.23422493e-05,\n",
       "         1.23493303e-05,  1.24403550e-05,  1.26367575e-05,\n",
       "         1.28928150e-05,  1.29823933e-05,  1.32081921e-05,\n",
       "         1.33242843e-05,  1.35911954e-05,  1.37838546e-05,\n",
       "         1.38362540e-05,  1.50894839e-05,  1.53655414e-05,\n",
       "         1.53806645e-05,  1.57876742e-05,  1.59298729e-05,\n",
       "         1.64700727e-05,  1.66368968e-05,  1.70397820e-05,\n",
       "         1.71393447e-05,  1.77218570e-05,  1.78008542e-05,\n",
       "         1.78153855e-05,  1.79911653e-05,  1.81057044e-05,\n",
       "         1.82670845e-05,  1.87680058e-05,  1.87854888e-05,\n",
       "         1.88055972e-05,  1.91994823e-05,  1.92460324e-05,\n",
       "         1.93666172e-05,  1.93765576e-05,  2.00829431e-05,\n",
       "         2.01713324e-05,  2.03233851e-05,  2.07084891e-05,\n",
       "         2.07310603e-05,  2.08336687e-05,  2.09243989e-05,\n",
       "         2.17924798e-05,  2.18651541e-05,  2.22861932e-05,\n",
       "         2.29825358e-05,  2.34235224e-05,  2.34602705e-05,\n",
       "         2.36746470e-05,  2.36762965e-05,  2.37124299e-05,\n",
       "         2.38784847e-05,  2.41108082e-05,  2.42621925e-05,\n",
       "         2.50230284e-05,  2.55834331e-05,  2.57222023e-05,\n",
       "         2.60995210e-05,  2.62234816e-05,  2.62826979e-05,\n",
       "         2.65185114e-05,  2.68001584e-05,  2.68154213e-05,\n",
       "         2.74530898e-05,  2.82252775e-05,  2.86922889e-05,\n",
       "         2.88713711e-05,  2.91011913e-05,  2.94431569e-05,\n",
       "         3.03451424e-05,  3.07442849e-05,  3.15063599e-05,\n",
       "         3.19802434e-05,  3.20102744e-05,  3.20653438e-05,\n",
       "         3.25127964e-05,  3.28065593e-05,  3.33533840e-05,\n",
       "         3.40999425e-05,  3.56785701e-05,  3.57281313e-05,\n",
       "         3.57504723e-05,  3.63617660e-05,  3.64668467e-05,\n",
       "         3.76491211e-05,  3.83893258e-05,  3.85996521e-05,\n",
       "         3.96227834e-05,  3.97982323e-05,  4.05277326e-05,\n",
       "         4.29552171e-05,  4.33596827e-05,  4.36741244e-05,\n",
       "         4.51318385e-05,  4.93272113e-05,  4.97264829e-05,\n",
       "         4.98763260e-05,  5.13245805e-05,  5.30532917e-05,\n",
       "         5.31659857e-05,  5.33768876e-05,  5.39199197e-05,\n",
       "         5.47435059e-05,  5.58687185e-05,  5.82463583e-05,\n",
       "         5.85253081e-05,  5.85277740e-05,  5.91040642e-05,\n",
       "         6.02664978e-05,  6.15405480e-05,  6.19299124e-05,\n",
       "         6.22386594e-05,  6.22620904e-05,  6.40380394e-05,\n",
       "         6.45219562e-05,  6.48312619e-05,  6.86887580e-05,\n",
       "         7.09682357e-05,  7.23430097e-05,  7.36599589e-05,\n",
       "         7.44571607e-05,  7.61889532e-05,  7.74647378e-05,\n",
       "         7.82405668e-05,  8.09589209e-05,  8.32965652e-05,\n",
       "         8.52274461e-05,  8.55416703e-05,  8.69206405e-05,\n",
       "         8.70876510e-05,  8.89987451e-05,  9.03837542e-05,\n",
       "         9.06430276e-05,  9.09603621e-05,  9.12273871e-05,\n",
       "         9.18391172e-05,  9.38536280e-05,  9.54863921e-05,\n",
       "         1.00773993e-04,  1.01101233e-04,  1.01431439e-04,\n",
       "         1.04304131e-04,  1.07614392e-04,  1.11300327e-04,\n",
       "         1.13013108e-04,  1.15093161e-04,  1.31713804e-04,\n",
       "         1.32353911e-04,  1.33414613e-04,  1.40832525e-04,\n",
       "         1.41476012e-04,  1.47613697e-04,  1.56571168e-04,\n",
       "         1.62002196e-04,  1.69469779e-04,  1.71709624e-04,\n",
       "         1.72517255e-04,  1.79533893e-04,  1.80900037e-04,\n",
       "         1.86982814e-04,  1.90150676e-04,  1.95292858e-04,\n",
       "         2.02012181e-04,  2.04381527e-04,  2.09735903e-04,\n",
       "         2.18348646e-04,  2.18422479e-04,  2.19485103e-04,\n",
       "         2.20952447e-04,  2.22195441e-04,  2.43651964e-04,\n",
       "         2.68942545e-04,  2.78862739e-04,  2.82188428e-04,\n",
       "         2.84884629e-04,  2.86292551e-04,  2.86537070e-04,\n",
       "         3.03750644e-04,  3.50348411e-04,  3.51210982e-04,\n",
       "         3.51497834e-04,  3.51732264e-04,  3.62697830e-04,\n",
       "         4.06712936e-04,  4.07372186e-04,  4.15733616e-04,\n",
       "         4.15798907e-04,  4.34015240e-04,  4.41548351e-04,\n",
       "         4.64969748e-04,  4.69173226e-04,  4.74719036e-04,\n",
       "         5.10674782e-04,  5.34580453e-04,  5.38425881e-04,\n",
       "         5.53140241e-04,  5.66630890e-04,  7.42458425e-04,\n",
       "         7.73623697e-04,  8.36033036e-04,  8.74423356e-04]])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(freq.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossfreq =cross_val_score(freq,otudf,y=wwfdf.Water,cv=10,n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMDS 20 dimensions as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = LogisticRegression(penalty=\"l1\",class_weight=\"balanced\",multi_class=\"ovr\",solver= \"liblinear\",max_iter=10000)\n",
    "freq.fit(Xn_train,y_train)\n",
    "freq.score(Xn_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.63877858,  0.        ,  2.80635959,  0.66580615, -4.96560675,\n",
       "        -1.01601738, -3.27691308,  0.        ,  0.        ,  0.        ,\n",
       "         3.776655  ,  0.15222217,  0.        ,  0.68760929, -1.29608284,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both liblinear and saga produced 90% accuracy on the test set when using nmds as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12121212121212122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1297709923664122"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.sum(y_test == \"Black\")/y_test.count())\n",
    "np.sum(y_train == \"Black\")/(y_train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3214814814814815"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pd.DataFrame(data= freq.coef_.T).describe()\n",
    "np.sum(freq.coef_ == 0)/np.size(freq.coef_)\n",
    "# 32% of coefficients go to zero. The biggest weight has a magnitude of the order of 10e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Bayesian logistic regression using pymc3\n",
    "\n",
    "I cant get it to work with either nmds or otutable, there might be something wrong with the code or with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def logit(x):\n",
    "    return(tt.exp(x)/(1+tt.exp(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train.bool = (y_train == \"White\")*1\n",
    "y_test.bool = (y_test == \"White\")*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [intercept, beta]\n",
      "Sampling 2 chains: 100%|██████████| 5000/5000 [00:15<00:00, 327.72draws/s]\n"
     ]
    }
   ],
   "source": [
    "niter=2000\n",
    "bay = pm.Model()\n",
    "with bay:\n",
    "    # Weights of the features have a laplace prior which is the same as l1 when minimising the loss function\n",
    "    lam = 1\n",
    "    beta = pm.Laplace(\"beta\",0,lam,shape = (np.array(Xn_train).shape[1],1))\n",
    "    inter = pm.Laplace(\"intercept\", 0, lam)\n",
    "\n",
    "    y_hat = tt.dot((Xn_train),beta) + inter\n",
    "    # Calculating the probability of water being white p(y=1|mu) so that we can get \n",
    "    # p(y|x,beta) ~ Binomial(y|mu)\n",
    "    mu =logit(y_hat)\n",
    "    y_obs = pm.Binomial(\"Y_obs\",n=np.ones(y_train.size),p = mu,observed = y_train.bool)\n",
    "    trace = pm.sample(niter,random_seed=11235,step =pm.NUTS() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spanashis/anaconda2/envs/py36/lib/python3.6/site-packages/pymc3/tuning/starting.py:61: UserWarning: find_MAP should not be used to initialize the NUTS sampler, simply call pymc3.sample() and it will automatically initialize NUTS in a better way.\n",
      "  warnings.warn('find_MAP should not be used to initialize the NUTS sampler, simply call pymc3.sample() and it will automatically initialize NUTS in a better way.')\n",
      "logp = -6,639.8, ||grad|| = 4.657: 100%|██████████| 37/37 [00:00<00:00, 729.30it/s]    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'beta': array([[-6.62310080e-06],\n",
       "        [ 4.16495878e-05],\n",
       "        [-8.97076413e-06],\n",
       "        [-5.26131853e-05],\n",
       "        [ 9.62803841e-06],\n",
       "        [-7.40750960e-05],\n",
       "        [ 2.24612320e-05],\n",
       "        [ 1.61002264e-05],\n",
       "        [ 1.80049943e-05],\n",
       "        [ 2.34860407e-05],\n",
       "        [ 1.12658329e-05],\n",
       "        [-2.22524733e-05],\n",
       "        [ 1.29637215e-05],\n",
       "        [ 1.00649396e-05],\n",
       "        [ 4.59244067e-07],\n",
       "        [ 4.94862295e-05],\n",
       "        [-1.78496617e-05],\n",
       "        [ 8.55649367e-06],\n",
       "        [-1.12016865e-05],\n",
       "        [-2.29966591e-05]]), 'intercept': array(1.9024686)}"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trace = pm.trace_to_dataframe(trace[niter//2:])\n",
    "betas =df_trace.mean(0)[0:-1]\n",
    "intercept = df_trace.mean(0)[-1]\n",
    "pm.find_MAP(model=bay)\n",
    "#bay.Y_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def predict(X,betas=betas,intercept=intercept):\n",
    "    \n",
    "    v = np.dot(X,betas) + intercept\n",
    "    return(np.exp(v)/(1+np.exp(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "3-S20b     True\n",
       "1-S02C     True\n",
       "3-S12b     True\n",
       "2-S17C     True\n",
       "2-S15B    False\n",
       "2-S16B     True\n",
       "2-S15E    False\n",
       "1-S03C     True\n",
       "3-S14f     True\n",
       "2-S19C     True\n",
       "3-S01b     True\n",
       "3-S18a     True\n",
       "3-S18d     True\n",
       "3-S10d    False\n",
       "3-S06c     True\n",
       "3-S21d     True\n",
       "3-S17d     True\n",
       "1-S06B     True\n",
       "3-S20a     True\n",
       "3-S21a     True\n",
       "3-S16a     True\n",
       "1-S05B     True\n",
       "2-S13A    False\n",
       "3-S05b     True\n",
       "3-S11d    False\n",
       "1-S02A     True\n",
       "2-S15C    False\n",
       "2-S14A    False\n",
       "2-S18A    False\n",
       "3-S20c     True\n",
       "          ...  \n",
       "1-S08B     True\n",
       "3-S18c     True\n",
       "3-S07c     True\n",
       "2-S15A    False\n",
       "3-S09c    False\n",
       "2-S14C     True\n",
       "1-S10A     True\n",
       "3-S12c     True\n",
       "3-S17A     True\n",
       "3-S04a     True\n",
       "2-S12A     True\n",
       "3-S03b     True\n",
       "2-S13D     True\n",
       "3-S03c     True\n",
       "3-S04d     True\n",
       "3-S15c     True\n",
       "1-S05C     True\n",
       "3-S03d     True\n",
       "3-S19c     True\n",
       "3-S01c     True\n",
       "1-S11C     True\n",
       "3-S09b     True\n",
       "3-S14g     True\n",
       "3-S15b     True\n",
       "2-S14B     True\n",
       "1-S04B     True\n",
       "2-S17B     True\n",
       "3-S17b     True\n",
       "3-S11b     True\n",
       "3-S20d    False\n",
       "Name: Water, Length: 131, dtype: bool"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predict(Xn_train) >= 0.5 )*1 ==y_train.bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n = 5 * np.ones(4)\n",
    "x = np.array([-0.896, -0.296, -0.053, 0.727])\n",
    "y = np.array([0, 1, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [beta, alpha]\n",
      "Sampling 2 chains: 100%|██████████| 3000/3000 [00:02<00:00, 1444.81draws/s]\n"
     ]
    }
   ],
   "source": [
    "niter=1000\n",
    "def invlogit(x):\n",
    "    return tt.exp(x) / (1 + tt.exp(x))\n",
    "\n",
    "with pm.Model() as model:\n",
    "    alpha = pm.Laplace('alpha', mu=0, b=1)\n",
    "    beta = pm.Laplace('beta',0,1)\n",
    "\n",
    "    p = invlogit(alpha + tt.dot(beta,x))\n",
    "    y_obs = pm.Binomial('y_obs', n=n, p=p, observed=y)\n",
    "\n",
    "    trace = pm.sample(niter, random_seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta -13.862943611198906\n",
      "intercept -0.6931471805599453\n",
      "Y_obs -11895.098765591549\n"
     ]
    }
   ],
   "source": [
    "for RV in bay.basic_RVs:\n",
    "    print(RV.name, RV.logp(bay.test_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfr_model(X, y):\n",
    "# Perform Grid-Search\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        param_grid={\n",
    "            'max_depth': range(3,7),\n",
    "            'n_estimators': (500, 1000,2000),\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0,                         n_jobs=-1)\n",
    "    \n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    #rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],random_state=False, verbose=False)\n",
    "# Perform K-Fold CV\n",
    "    #scores = cross_val_score(rfr, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spanashis/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "bestpar =rfr_model(X_train,y_train.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9393939393939394"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr =RandomForestClassifier(max_depth=None, n_estimators=10000,random_state=True, verbose=True)\n",
    "rfr.fit(X_train,y_train)\n",
    "rfr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest with 10000 trees, max depth set to None and random_state set to True produces an accuracy of 93%, which is equivalent to that of the logistic regression with the saga solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9696969696969697"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.fit(Xn_train,y_train)\n",
    "rfr.score(Xn_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the nmds axis we get an accuracy of 96.7% (which is a single mistake!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rfr.predict(X_test) == y_test)/y_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    4.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "crossrfr =cross_val_score(rfr,otudf,y=wwfdf.Water,cv=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
